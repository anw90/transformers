{
    "fsdp_transformer_layer_cls_to_wrap": [
        "GPT2Block"
    ],
    "xla": true,
    "xla_fsdp_settings": {
        "compute_dtype": "float16",
        "buffer_dtype": "float16",
        "pin_layout_in_collective_ops": false,
        "flatten_parameters": true
    },
    "xla_fsdp_grad_ckpt": true
}